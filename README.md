# ğŸš€ Open Lakehouse integrated with Data Quality Mangagement Framework
### A Fully Local, Open-Source Lakehouse for Ingestion, Storage, Processing, and Orchestration

This project implements a complete **Open Lakehouse architecture** using 100% open-source technologies. It is designed for end-to-end data ingestion, processing, storage, orchestration, and analytics - all running locally via Docker.

The architecture follows the **Medallion Model (Bronze â†’ Silver â†’ Gold)** and provides a unified environment for ETL/ELT, metadata management, SQL analytics, and data quality pipelines.

### Data Quality Mangagement Framework
The Data Quality Management Framework is tightly integrated into the Lakehouse to ensure trusted, reliable data.
#### Introduction to Project: 
[NghiÃªn cá»©u vÃ  xÃ¢y dá»±ng Lakehouse káº¿t há»£p Framework Quáº£n lÃ½ Cháº¥t lÆ°á»£ng Dá»¯ liá»‡u](https://youtu.be/I_r0h9v-vzA)
#### Project Demo: 
[DEMO-NghiÃªn cá»©u vÃ  xÃ¢y dá»±ng Lakehouse káº¿t há»£p Framework Quáº£n lÃ½ Cháº¥t lÆ°á»£ng Dá»¯ liá»‡u](https://youtu.be/FE0A83WuH-k)



---
## ğŸ§± Lakehouse Architecture Summary

### ğŸ—„ï¸ **Data Lake Storage**
- **MinIO** - S3-compatible object storage for all Bronze, Silver, and Gold layers.

### ğŸ“ **Supported File Formats**
- **Parquet**, **ORC**, **CSV**, **JSON**, and other semi-structured formats.

### ğŸ§© **Open Table Format**
- **Delta Lake** - ACID transactions, schema evolution, time travel, and optimized Parquet storage.

### ğŸ—ƒï¸ **Metastore**
- **Hive Metastore** - central metadata catalog for Delta Lake and Trino.

### ğŸ’»**Processing Engine**
- **Apache Spark + Delta Lake** - Performs ETL/ELT jobs and medallion transformations.

### âš™ï¸ **Orchestration Layer**
- **Apache Airflow 3.1**  - For automate the process: Schedules ingestion, processing, and enrichment jobs.

### ğŸ“Š **Compute Engine**
- **Trino** - distributed SQL query engine for analytics across the entire Lakehouse.

### ğŸ³ **Containerization & Deployment**
- **Docker & Docker Compose** - orchestrates MinIO, Hive Metastore, Trino, Spark, and Airflow into a fully local Lakehouse environment.

  
## ğŸ›¡ï¸ Data Quality Management Framework

The Data Quality Management Framework is fully integrated into the Lakehouse to ensure **reliable, trustworthy data**.

### ğŸ” Automated Data Quality Checks
- Completeness - Data is considered complete when all the data required for a particular use is present and available to be used. 
- Validity - Validity is defined as the extent to which the data conforms to the expected format, type, and range.

Prevents data quality issues from propagating to downstream layers.

---

### ğŸš¨ Alerting & Monitoring
- Automatically detects data quality violations
- Sends alerts when rules are breached through email
- Highlights abnormal records for investigation

